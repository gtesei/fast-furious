---
title: "CAP_11_Measur_Perf_in_Class_models"
author: "Gino Tesei"
date: "October 8, 2014"
output: html_document
---

The R packages AppliedPredictiveModeling, caret, klaR, MASS, pROC, and
randomForest will be utilized in this section.
For illustration, the simulated data set shown in Fig. 11.1 will be used in
this section. To create these data, the quadBoundaryFunc function in the AppliedPredictiveModeling package is used to generate the predictors and outcomes:

```{r}
library(AppliedPredictiveModeling)
set.seed(975)
simulatedTrain <- quadBoundaryFunc(500)
simulatedTest <- quadBoundaryFunc(1000)
head(simulatedTrain)
head(simulatedTest)
```


The random forest and quadratic discriminant models will be fit to the data:

```{r}
library(randomForest)
rfModel <- randomForest(class ~ X1 + X2, data = simulatedTrain,  ntree = 2000)

library(MASS) ## for the qda() function
qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)
```

The output of the predict function for qda objects includes both the predicted
classes (in a slot called class) and the associated probabilities are in a matrix
called posterior. For the QDA model, predictions will be created for the
training and test sets. Later in this section, the training set probabilities
will be used in an additional model to calibrate the class probabilities. The
calibration will then be applied to the test set probabilities:

```{r}
qdaTrainPred <- predict(qdaModel, simulatedTrain)
names(qdaTrainPred)
head(qdaTrainPred$class)
head(qdaTrainPred$posterior)

qdaTestPred <- predict(qdaModel, simulatedTest)
simulatedTrain$QDAprob <- qdaTrainPred$posterior[,"Class1"]
simulatedTest$QDAprob <- qdaTestPred$posterior[,"Class1"]

rfTestPred <- predict(rfModel, simulatedTest, type = "prob")
head(rfTestPred)

simulatedTest$RFprob <- rfTestPred[,"Class1"]
simulatedTest$RFclass <- predict(rfModel, simulatedTest)
```

# Sensitivity and Specificity
caret has functions for computing sensitivity and specificity. These functions
require the user to indicate the role of each of the classes:
```{r}
library(caret)
# Class 1 will be used as the event of interest
sensitivity(data = simulatedTest$RFclass, reference = simulatedTest$class, positive = "Class1")
specificity(data = simulatedTest$RFclass, reference = simulatedTest$class, negative = "Class2")

```

Predictive values can also be computed either by using the prevalence found in the data set (46 %) or by using prior judgement:

```{r}
posPredValue(data = simulatedTest$RFclass, reference = simulatedTest$class, positive = "Class1")
negPredValue(data = simulatedTest$RFclass, reference = simulatedTest$class, positive = "Class2")

# Change the prevalence manually
posPredValue(data = simulatedTest$RFclass, reference = simulatedTest$class, positive = "Class1", prevalence = .9)

```


# Confusion Matrix
There are several functions in R to create the confusion matrix. The
confusionMatrix function in the caret package produces the table and associated statistics:

```{r}
confusionMatrix(data = simulatedTest$RFclass, reference = simulatedTest$class, positive = "Class1")

```

There is also an option in this function to manually set the prevalence. If there
were more than two classes, the sensitivity, specificity, and similar statistics
are calculated on a “one-versus-all” basis (e.g., the first class versus a pool of
classes two and three).

#Receiver Operating Characteristic Curves
The pROC package (Robin et al. 2011) can create the curve and derive various
statistics.
First, an R object must be created that contains the relevant
information using the pROC function roc. The resulting object is then used
to generate the ROC curve or calculate the area under the curve. For example,

```{r}
## This function assumes that the second
## class is the event of interest, so we
## reverse the labels.
library(pROC)
rocCurve <- roc(response = simulatedTest$class, predictor = simulatedTest$RFprob, levels = rev(levels(simulatedTest$class)))
auc(rocCurve)
ci.roc(rocCurve)
```

We can also use the plot function to produce the ROC curve itself:
```{r}
plot(rocCurve, legacy.axes = TRUE)
```


#Lift Charts
The lift curve can be created using the lift function in the caret package.
It takes a formula as the input where the true class is on the left-hand side
of the formula, and one or more columns for model class probabilities are on
the right. For example, to produce a lift plot for the random forest and QDA
test set probabilities,

```{r}
labs <- c(RFprob = "Random Forest", QDAprob = "Quadratic Discriminant Analysis")
liftCurve <- lift(class ~ RFprob + QDAprob, data = simulatedTest, labels = labs)
liftCurve
```

To plot two lift curves, the xyplot function is used to create a lattice plot:

```{r}
## Add lattice options to produce a legend on top
xyplot(liftCurve, auto.key = list(columns = 2, lines = TRUE, points = FALSE))
```


#Calibrating Probabilities
Calibration plots as described above are available in the calibration.plot
function in the PresenceAbsence package and in the caret function calibration
(details below). The syntax for the calibration function is similar to the lift
function:

```{r}
calCurve <- calibration(class ~ RFprob + QDAprob, data = simulatedTest)
calCurve
xyplot(calCurve, auto.key = list(columns = 2))
```
Figure also shows this plot. An entirely different approach to calibration
plots that model the observed event rate as a function of the class probabilities
can be found in the calibrate.plot function of the gbm package.
To recalibrate the QDA probabilities, a post-processing model is created
that models the true outcome as a function of the class probability. To fit
a sigmoidal function, a logistic regression model is used (see Sect. 12.2 for
more details) via the glm function in base R. This function is an interface
to a broad set of methods called generalized linear models (Dobson 2002),
which includes logistic regression. To fit the model, the function requires the
family argument to specify the type of outcome data being modeled. Since
our outcome is a discrete category, the binomial distribution is selected:

```{r}
## The glm() function models the probability of the second factor
## level, so the function relevel() is used to temporarily reverse the
## factors levels.
sigmoidalCal <- glm(relevel(class, ref = "Class2") ~ QDAprob, data = simulatedTrain, family = binomial)
coef(summary(sigmoidalCal)) 

```

The corrected probabilities are created by taking the original model and
applying Eq. 11.1 with the estimated slope and intercept. In R, the predict
function can be used:

```{r}
sigmoidProbs <- predict(sigmoidalCal, newdata = simulatedTest[,"QDAprob", drop = FALSE], type = "response")
simulatedTest$QDAsigmoid <- sigmoidProbs

``` 


The Bayesian approach for calibration is to treat the training set class probabilities to estimate the probabilities
(see Eq. 13.5 on page 354). In R, the naives Bayes model function NaiveBayes in the klaR
package can be used for the computations:

```{r}
library(klaR)
BayesCal <- NaiveBayes(class ~ QDAprob, data = simulatedTrain, usekernel = TRUE)
## Like qda(), the predict function for this model creates
## both the classes and the probabilities
BayesProbs <- predict(BayesCal, newdata = simulatedTest[, "QDAprob", drop = FALSE])
simulatedTest$QDABayes <- BayesProbs$posterior[, "Class1"]
## The probability values before and after calibration
head(simulatedTest[, c(5:6, 8, 9)])
``` 


The option usekernel = TRUE allows a flexible function to model the probability distribution of the class probabilities.
These new probabilities are evaluated using another plot:

```{r}
calCurve2 <- calibration(class ~ QDAprob + QDABayes + QDAsigmoid, data = simulatedTest)
xyplot(calCurve2)
``` 



