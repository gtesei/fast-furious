---
title: "CAP_11_Measur_Perf_in_Class_models"
author: "Gino Tesei"
date: "October 8, 2014"
output: html_document
---

The R packages AppliedPredictiveModeling, caret, klaR, MASS, pROC, and
randomForest will be utilized in this section.
For illustration, the simulated data set shown in Fig. 11.1 will be used in
this section. To create these data, the quadBoundaryFunc function in the AppliedPredictiveModeling package is used to generate the predictors and outcomes:
> library(AppliedPredictiveModeling)
> set.seed(975)
> simulatedTrain <- quadBoundaryFunc(500)
> simulatedTest <- quadBoundaryFunc(1000)
> head(simulatedTrain)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 266). Springer. Kindle Edition. 

The random forest and quadratic discriminant models will be fit to the data:
> library(randomForest)
> rfModel <- randomForest(class ~ X1 + X2,
+ data = simulatedTrain,
+ ntree = 2000)
> library(MASS) ## for the qda() function
> qdaModel <- qda(class ~ X1 + X2, data = simulatedTrain)
The output of the predict function for qda objects includes both the predicted
classes (in a slot called class) and the associated probabilities are in a matrix
called posterior. For the QDA model, predictions will be created for the
training and test sets. Later in this section, the training set probabilities
will be used in an additional model to calibrate the class probabilities. The
calibration will then be applied to the test set probabilities:
> qdaTrainPred <- predict(qdaModel, simulatedTrain)
> names(qdaTrainPred)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 

> head(qdaTrainPred$class)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 
> head(qdaTrainPred$posterior)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 
> qdaTestPred <- predict(qdaModel, simulatedTest)
> simulatedTrain$QDAprob <- qdaTrainPred$posterior[,"Class1"]
> simulatedTest$QDAprob <- qdaTestPred$posterior[,"Class1"]

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 


> rfTestPred <- predict(rfModel, simulatedTest, type = "prob")
> head(rfTestPred)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 

> simulatedTest$RFprob <- rfTestPred[,"Class1"]
> simulatedTest$RFclass <- predict(rfModel, simulatedTest)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 267). Springer. Kindle Edition. 

Sensitivity and Specificity
caret has functions for computing sensitivity and specificity. These functions
require the user to indicate the role of each of the classes:
> # Class 1 will be used as the event of interest
> sensitivity(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ positive = "Class1")

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 

> specificity(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ negative = "Class2")

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 

Predictive values can also be computed either by using the prevalence found
in the data set (46 %) or by using prior judgement:
> posPredValue(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ positive = "Class1")

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 


> negPredValue(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ positive = "Class2")

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 

> # Change the prevalence manually
> posPredValue(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ positive = "Class1",
+ prevalence = .9)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 


Confusion Matrix

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 


There are several functions in R to create the confusion matrix. The
confusionMatrix function in the caret package produces the table and associated statistics:
> confusionMatrix(data = simulatedTest$RFclass,
+ reference = simulatedTest$class,
+ positive = "Class1")
Confusion Matrix and Statistics

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 268). Springer. Kindle Edition. 


There is also an option in this function to manually set the prevalence. If there
were more than two classes, the sensitivity, specificity, and similar statistics
are calculated on a “one-versus-all” basis (e.g., the first class versus a pool of
classes two and three).
Receiver Operating Characteristic Curves
The pROC package (Robin et al. 2011) can create the curve and derive various
statistics.
6
First, an R object must be created that contains the relevant
information using the pROC function roc. The resulting object is then used
to generate the ROC curve or calculate the area under the curve. For example,

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 269). Springer. Kindle Edition. 



> library(pROC)
> rocCurve <- roc(response = simulatedTest$class,
+ predictor = simulatedTest$RFprob,
+ ## This function assumes that the second
+ ## class is the event of interest, so we
+ ## reverse the labels.
+ levels = rev(levels(simulatedTest$class)))
From this object, we can produce statistics (such as the area under the ROC
curve and its confidence interval):
> auc(rocCurve)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 269). Springer. Kindle Edition. 


> ci.roc(rocCurve)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 269). Springer. Kindle Edition. 

We can also use the plot function to produce the ROC curve itself:
> plot(rocCurve, legacy.axes = TRUE)
> ## By default, the x-axis goes backwards, used
> ## the option legacy.axes = TRUE to get 1-spec
> ## on the x-axis moving from 0 to 1
>
> ## Also, another curve can be added using
> ## add = TRUE the next time plot.auc is used.
Figure 11.8 shows the results of this function call.

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 270). Springer. Kindle Edition. 



Lift Charts
The lift curve can be created using the lift function in the caret package.
It takes a formula as the input where the true class is on the left-hand side
of the formula, and one or more columns for model class probabilities are on
the right. For example, to produce a lift plot for the random forest and QDA
test set probabilities,

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 270). Springer. Kindle Edition. 



> labs <- c(RFprob = "Random Forest",
+ QDAprob = "Quadratic Discriminant Analysis")
> liftCurve <- lift(class ~ RFprob + QDAprob, data = simulatedTest,
+ labels = labs)
> liftCurve

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 271). Springer. Kindle Edition. 

To plot two lift curves, the xyplot function is used to create a lattice plot:
> ## Add lattice options to produce a legend on top
> xyplot(liftCurve,
+ auto.key = list(columns = 2,
+ lines = TRUE,
+ points = FALSE))

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 271). Springer. Kindle Edition. 


Calibrating Probabilities
Calibration plots as described above are available in the calibration.plot
function in the PresenceAbsence package and in the caret function calibration
(details below). The syntax for the calibration function is similar to the lift
function:
> calCurve <- calibration(class ~ RFprob + QDAprob, data = simulatedTest)
> calCurve

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 271). Springer. Kindle Edition. 



> xyplot(calCurve, auto.key = list(columns = 2))
Figure 11.9 also shows this plot. An entirely different approach to calibration
plots that model the observed event rate as a function of the class probabilities
can be found in the calibrate.plot function of the gbm package.
To recalibrate the QDA probabilities, a post-processing model is created
that models the true outcome as a function of the class probability. To fit
a sigmoidal function, a logistic regression model is used (see Sect. 12.2 for
more details) via the glm function in base R. This function is an interface
to a broad set of methods called generalized linear models (Dobson 2002),
which includes logistic regression. To fit the model, the function requires the

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 271). Springer. Kindle Edition. 



family argument to specify the type of outcome data being modeled. Since
our outcome is a discrete category, the binomial distribution is selected:
> ## The glm() function models the probability of the second factor
> ## level, so the function relevel() is used to temporarily reverse the
> ## factors levels.

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 272). Springer. Kindle Edition. 


> sigmoidalCal <- glm(relevel(class, ref = "Class2") ~ QDAprob,
+ data = simulatedTrain,
+ family = binomial)
> coef(summary(sigmoidalCal))

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 273). Springer. Kindle Edition. 


The corrected probabilities are created by taking the original model and
applying Eq. 11.1 with the estimated slope and intercept. In R, the predict
function can be used:
> sigmoidProbs <- predict(sigmoidalCal,
+ newdata = simulatedTest[,"QDAprob", drop = FALSE],
+ type = "response")
> simulatedTest$QDAsigmoid <- sigmoidProbs

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 273). Springer. Kindle Edition. 


The Bayesian approach for calibration is to treat the training set class probabilities to estimate the probabilities P r[X] and P r[X|Y = C] (see Eq. 13.5
on page 354). In R, the na¨ıve Bayes model function NaiveBayes in the klaR
package can be used for the computations:
> BayesCal <- NaiveBayes(class ~ QDAprob, data = simulatedTrain,
+ usekernel = TRUE)
> ## Like qda(), the predict function for this model creates
> ## both the classes and the probabilities
> BayesProbs <- predict(BayesCal,
+ newdata = simulatedTest[, "QDAprob", drop = FALSE])
> simulatedTest$QDABayes <- BayesProbs$posterior[, "Class1"]
> ## The probability values before and after calibration
> head(simulatedTest[, c(5:6, 8, 9)])

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 273). Springer. Kindle Edition. 


The option usekernel = TRUE allows a flexible function to model the probability distribution of the class probabilities.
These new probabilities are evaluated using another plot:
> calCurve2 <- calibration(class ~ QDAprob + QDABayes + QDAsigmoid,
+ data = simulatedTest)
> xyplot(calCurve2)

Kuhn, Max; Johnson, Kjell (2013-05-17). Applied Predictive Modeling (Page 273). Springer. Kindle Edition. 


