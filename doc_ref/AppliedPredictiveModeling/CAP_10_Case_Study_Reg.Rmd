---
title: "CAP_10_Case_Study_Reg"
author: "Gino Tesei"
date: "September 20, 2014"
output: html_document
---

This section uses functions from the caret, desirability, Hmisc, and plyr
packages.
The concrete data can be found in the UCI Machine Learning repository.
The AppliedPredictiveModeling package contains the original data
(in amounts) and an alternate version that has the mixture proportions:

```{r}
library(caret)
library(AppliedPredictiveModeling)
data(concrete)
str(concrete)

```
Table 10.1 was created using the describe function in the Hmisc package,
and Fig. 10.1 was create using the featurePlot function in caret:

```{r}
featurePlot(x = concrete[, -9], y = concrete$CompressiveStrength,
            ## Add some space between the panels
            between = list(x = 1, y = 1),
            ## Add a background grid ('g') and a smoother ('smooth')
            type = c("g", "p", "smooth")) 

```

The code for averaging the replicated mixtures and splitting the data into
training and test sets is

```{r}
library(plyr)
averaged <- ddply(mixtures, .(Cement, BlastFurnaceSlag, FlyAsh, Water,Superplasticizer, CoarseAggregate,FineAggregate, Age),
                  function(x) c(CompressiveStrength = mean(x$CompressiveStrength)))

set.seed(975)
forTraining <- createDataPartition(averaged$CompressiveStrength, p = 3/4)[[1]]
trainingSet <- averaged[ forTraining,]
testSet <- averaged[-forTraining,]

```

To fit the linear models with the expanded set of predictors, such as interactions, a specific model formula was created. The dot in the formula below is shorthand for all predictors and ** (.)^2 expands into a model with all the linear terms and all two-factor interactions **. The quadratic terms are created manually and are encapsulated inside the I() function. This “as-is” function tells R that the squaring of the predictors should be done arithmetically (and not symbolically). The formula is first created as a character string using the paste command, then is converted to a bona fide R formula.

```{r}
modFormula <- paste( "CompressiveStrength ~ (.)^2 + I(Cement^2) + ", "I(BlastFurnaceSlag^2) + I(FlyAsh^2) + I(Water^2) +", 
                    " I(Superplasticizer^2) + I(CoarseAggregate^2) + ",
                    "I(FineAggregate^2) + I(Age^2)" )
modFormula <- as.formula(modFormula)

```
Each model used repeated 10-fold cross-validation and is specified with
the trainControl function:

```{r}
controlObject <- trainControl(method = "repeatedcv", repeats = 5, number = 10)
```

To create the exact same folds, the random number generator is reset to a
common seed prior to running train. For example, to fit the linear regression
model:

```{r}
set.seed(669)
linearReg <- train(modFormula, data = trainingSet, method = "lm", trControl = controlObject) 
linearReg
```

The output shows that 44 predictors were used, indicating the expanded
model formula was used.
The other two linear models were created with:

```{r}
set.seed(669)
plsModel <- train(modFormula, data = trainingSet, method = "pls", preProc = c("center", "scale"), tuneLength = 15, trControl = controlObject)

enetGrid <- expand.grid(.lambda = c(0, .001, .01, .1), .fraction = seq(0.05, 1, length = 20))
set.seed(669)
enetModel <- train(modFormula, data = trainingSet, method = "enet", preProc = c("center", "scale"), tuneGrid = enetGrid, trControl = controlObject)

```

MARS, neural networks, and SVMs were created as follows:

```{r}
set.seed(669)
earthModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "earth", 
                    tuneGrid = expand.grid(.degree = 1, .nprune = 2:25), trControl = controlObject)

set.seed(669)
svmRModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "svmRadial",
                   tuneLength = 15, preProc = c("center", "scale"),  trControl = controlObject)

nnetGrid <- expand.grid(.decay = c(0.001, .01, .1), .size = seq(1, 27, by = 2), .bag = FALSE)
set.seed(669)
nnetModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "avNNet", 
                   tuneGrid = nnetGrid, preProc = c("center", "scale"), linout = TRUE, 
                   trace = FALSE, maxit = 1000, trControl = controlObject)

```

The regression and model trees were similarly created:
```{r}
set.seed(669)
rpartModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "rpart", tuneLength = 30, trControl = controlObject)

set.seed(669)
ctreeModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "ctree", tuneLength = 10, trControl = controlObject)

set.seed(669)
mtModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "M5", trControl = controlObject)

```

The following code creates the remaining model objects:

```{r}
set.seed(669)
treebagModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "treebag", trControl = controlObject)

set.seed(669)
rfModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "rf", tuneLength = 10, ntrees = 1000, 
                 importance = TRUE, trControl = controlObject)

gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by = 50), .shrinkage = c(0.01, 0.1))
set.seed(669)
gbmModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "gbm", tuneGrid = gbmGrid, 
                  verbose = FALSE, trControl = controlObject)

cubistGrid <- expand.grid(.committees = c(1, 5, 10, 50, 75, 100), .neighbors = c(0, 1, 3, 5, 7, 9))
set.seed(669)
cbModel <- train(CompressiveStrength ~ ., data = trainingSet, method = "cubist", 
                 tuneGrid = cubistGrid, trControl = controlObject)

```

The resampling results for these models were collected into a single object
using caret’s resamples function. This object can then be used for visualizations or to make formal comparisons between the models.

```{r}
allResamples <- resamples(list("Linear Reg" = lmModel, 
                                 "PLS" = plsModel, 
                                 "Elastic Net" = enetModel, 
                                 "MARS" = earthModel, 
                                 "SVM" = svmRModel, 
                                 "Neural Networks" = nnetModel, 
                                 "CART" = rpartModel, 
                                 "Cond Inf Tree" = ctreeModel, 
                                 "Bagged Tree" = treebagModel, 
                                 "Boosted Tree" = gbmModel, 
                                 "Random Forest" = rfModel, 
                                 "Cubist" = cbModel))

```

Figure 10.2 was created from this object as 

```{r}
## Plot the RMSE values
parallelPlot(allResamples)
## Using R-squared:
parallelplot(allResamples, metric = "Rsquared")
```

Other visualizations of the resampling results can also be created (see ?xyplot.resamples for other options).
The test set predictions are achieved using a simple application of the predict function:

```{r}
nnetPredictions <- predict(nnetModel, testData)
gbmPredictions <- predict(gbmModel, testData)
cbPredictions <- predict(cbModel, testData)
```

To predict optimal mixtures, we first use the 28-day data to generate a set of random starting points from the training set.
Since distances between the formulations will be used as a measure of dissimilarity, the data are pre-processed to have the same mean and variance for each predictor. After this, a single random mixture is selected to initialize the maximum dissimilarity sampling process:

```{r}
age28Data <- subset(trainingData, Age == 28)
## Remove the age and compressive strength columns and
## then center and scale the predictor columns
pp1 <- preProcess(age28Data[, -(8:9)], c("center", "scale"))
scaledTrain <- predict(pp1, age28Data[, 1:7])

set.seed(91)
startMixture <- sample(1:nrow(age28Data), 1)
starters <- scaledTrain[startMixture, 1:7]
```

After this, the maximum dissimilarity sampling method from Sect. 4.3 selects 14 more mixtures to complete a diverse set of starting points for the search algorithms:

```{r}
pool <- scaledTrain
index <- maxDissim(starters, pool, 14)
startPoints <- c(startMixture, index)
starters <- age28Data[startPoints,1:7]
```
 
Since all seven mixture proportions should add to one, the search procedures will conduct the search without one ingredient (water), and the water proportion will be determined by the sum of the other six ingredient proportions.
Without this step, the search procedures would pick candidate mixture values that would not add to one.
 
```{r}
## Remove water
startingValues <- starters[, -4]
```

To maximize the compressive strength, the R function optim searches the mixture space for optimal formulations. A custom R function is needed to
translate a candidate mixture to a prediction. This function can find settings to minimize a function, so it will return the negative of the compressive strength. The function below checks to make sure that (a) the proportions are between 0 and 1 and (b) the proportion of water does not fall below 5 %. If these conditions are violated, the function returns a large positive number which the search procedure will avoid (as optim is for minimization).

```{r}
## The inputs to the function are a vector of six mixture proportions
## (in argument 'x') and the model used for prediction ('mod')
modelPrediction <- function(x, mod) {
  ## Check to make sure the mixture proportions are
  ## in the correct range
  if(x[1] < 0 | x[1] > 1) return(10^38)
  if(x[2] < 0 | x[2] > 1) return(10^38)
  if(x[3] < 0 | x[3] > 1) return(10^38)
  if(x[4] < 0 | x[4] > 1) return(10^38)
  if(x[5] < 0 | x[5] > 1) return(10^38)
  if(x[6] < 0 | x[6] > 1) return(10^38)
  
  ## Determine the water proportion
  
  x <- c(x, 1 - sum(x))
  
  ## Check the water range
  if(x[7] < 0.05) return(10^38)
  
  ## Convert the vector to a data frame, assign names
  ## and fix age at 28 days
  tmp <- as.data.frame(t(x))
  names(tmp) <- c('Cement','BlastFurnaceSlag','FlyAsh', 'Superplasticizer','CoarseAggregate', 'FineAggregate', 'Water')
  tmp$Age <- 28
  
  ## Get the model prediction, square them to get back to the
  ## original units, then return the negative of the result
  -predict(mod, tmp)
  }

```

First, the Cubist model is used:
```{r}
## Remove water
startingValues <- starters[, -4]
```

To maximize the compressive strength, the R function optim searches the mixture space for optimal formulations. A custom R function is needed to
translate a candidate mixture to a prediction. This function can find settings to minimize a function, so it will return the negative of the compressive strength. The function below checks to make sure that (a) the proportions are between 0 and 1 and (b) the proportion of water does not fall below 5 %. If these conditions are violated, the function returns a large positive number which the search procedure will avoid (as optim is for minimization).

```{r}
cbResults <- startingValues
cbResults$Water <- NA
cbResults$Prediction <- NA
## Loop over each starting point and conduct the search
for(i in 1:nrow(cbResults)) {
  results <- optim(unlist(cbResults[i,1:6]),
                   modelPrediction,
                   method = "Nelder-Mead",
                   ## Use method = 'SANN' for simulated annealing
                   control=list(maxit=5000),
                   ## The next option is passed to the
                   ## modelPrediction() function
                   mod = cbModel)
  
  ## Save the predicted compressive strength
  cbResults$Prediction[i] <- -results$value
  
  ## Also save the final mixture values
  cbResults[i,1:6] <- results$par
  }

## Calculate the water proportion
cbResults$Water <- 1 - apply(cbResults[,1:6], 1, sum)

## Keep the top three mixtures
cbResults <- cbResults[order(-cbResults$Prediction),][1:3,]
cbResults$Model <- "Cubist"
```

We then employ the same process for the neural network model:

```{r}
nnetResults <- startingValues
nnetResults$Water <- NA
nnetResults$Prediction <- NA
for(i in 1:nrow(nnetResults)) {
  results <- optim(unlist(nnetResults[i, 1:6,]),
                   modelPrediction,
                   method = "Nelder-Mead",
                   control=list(maxit=5000),
                   
                   mod = nnetModel)
  nnetResults$Prediction[i] <- -results$value
  nnetResults[i,1:6] <- results$par
  }

nnetResults$Water <- 1 - apply(nnetResults[,1:6], 1, sum)
nnetResults <- nnetResults[order(-nnetResults$Prediction),][1:3,]
nnetResults$Model <- "NNet"
``` 

To create Fig. 10.4, PCA was conducted on the 28-day-old mixtures and the six predicted mixtures were projected. The components are combined and plotted:

```{r}
## Run PCA on the data at 28\,days
pp2 <- preProcess(age28Data[, 1:7], "pca")

## Get the components for these mixtures
pca1 <- predict(pp2, age28Data[, 1:7])
pca1$Data <- "Training Set"

## Label which data points were used to start the searches
pca1$Data[startPoints] <- "Starting Values"

## Project the new mixtures in the same way (making sure to
## re-order the columns to match the order of the age28Data object).
pca3 <- predict(pp2, cbResults[, names(age28Data[, 1:7])])
pca3$Data <- "Cubist"

pca4 <- predict(pp2, nnetResults[, names(age28Data[, 1:7])])
pca4$Data <- "Neural Network"

## Combine the data, determine the axis ranges and plot
pcaData <- rbind(pca1, pca3, pca4)
pcaData$Data <- factor(pcaData$Data, levels = c("Training Set","Starting Values", "Cubist","Neural Network"))

lim <- extendrange(pcaData[, 1:2])
xyplot(PC2 ~ PC1, data = pcaData, groups = Data, auto.key = list(columns = 2), xlim = lim, ylim = lim, type = c("g", "p"))
``` 

Desirability functions can be calculated with the desirability package. The functions dMin and dMax can be used to create desirability function curve definitions for minimization and maximization, respectively.

