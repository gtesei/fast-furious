---
title: "CAP_19_Intro_Feat_Sel"
author: "Gino Tesei"
date: "September 19, 2014"
output: html_document
---
This section discusses data and/or functions from the following packages:
AppliedPredictiveModeling, caret, klaR, leaps, MASS, pROC, rms, and stats.
The data are contained in the AppliedPredictiveModeling package. The data
objects consist of a data frame of predictors called predictors and a factor
vector of class values called diagnosis (with levels impaired and control). The
following code was used to prepare the data for analysis:
```{r}
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
## Manually create new dummy variables
predictors$E2 <- predictors$E3 <- predictors$E4 <- 0
predictors$E2[grepl("2", predictors$Genotype)] <- 1
predictors$E3[grepl("3", predictors$Genotype)] <- 1
predictors$E4[grepl("4", predictors$Genotype)] <- 1

## Split the data using stratified sampling
set.seed(730)
split <- createDataPartition(diagnosis, p = .8, list = FALSE)
## Combine into one data frame
adData <- predictors
adData$Class <- diagnosis
training <- adData[ split, ]
testing <- adData[-split, ]
## Save a vector of predictor variable names
predVars <- names(adData)[!(names(adData) %in% c("Class", "Genotype"))]

## Compute the area under the ROC curve, sensitivity, specificity,
## accuracy and Kappa
fiveStats <- function(...) c(twoClassSummary(...), defaultSummary(...))
#Create resampling data sets to use for all models
set.seed(104)
index <- createMultiFolds(training$Class, times = 5)
## Create a vector of subset sizes to evaluate
varSeq <- seq(1, length(predVars)-1, by = 2)

```
# Forward, Backward, and Stepwise Selection
There are several R functions for this class of wrappers:

* step in the stats package can be used to search for appropriate subsets for
linear regression and generalized linear models (from the lm and glm functions, respectively). The direction argument controls the search method(e.g. “both,” “backward,” or “forward”). A more general function is the
stepAIC function in the MASS package, which can handle additional model
types. In either case, the AIC statistic (or its variants) is used as the
objective function.

* The fastbw function in the rms package conducts similar searches but has
the optional but unrecommended choice of using p-values as the objective
function.
* The regsubsets function in the leaps package has similar functionality.
* The klaR package contains the stepclass function than searches the predictor space for models that maximize cross-validated accuracy rates. Thefunction has built-in methods for several models, such as lda, but can be
more broadly generalized.

The caret package function train has wrappers for leaps, stepAIC, and
stepclass, so that the entire feature selection process can be resampled and
the risk of selection bias is reduced.
For example, to use stepAIC with logistic regression, the function takes an
initial model as input. To illustrate the function, a small model is used:

```{r}
initial <- glm(Class ~ tau + VEGF + E4 + IL_3, data = training,family = binomial)
library(MASS)
stepAIC(initial, direction = "both")

```
The function returns a glm object with the final predictor set. The other
functions listed above use similar strategies.

# Recursive Feature Elimination
The caret and varSelRF packages contain functions for recursive feature elimination. While the varSelRF function in the varSelRF is specific to random
forests, the rfe function in caret is a general framework for any predictive
model. For the latter, there are predefined functions for random forests, linear
discriminant analysis, bagged trees, na¨ıve Bayes, generalized linear models,
linear regression models, and logistic regression. The random forest functions
are in a list called rfFuncs:

```{r}
## The built-in random forest functions are in rfFuncs.
str(rfFuncs)
```

Each of these function defines a step in Algorithm 19.2:

* The summary function defines how the predictions will be evaluated (Line 10
in Algorithm 19.2).
* The fit function allows the user to specify the model and conduct parameter tuning (Lines 19.2, 6, and 12).
* The pred function generates predictions for new samples.
* The rank function generates variable importance measures (Line 2).
* The selectSize function chooses the appropriate predictor subset size
(Line 11).
* The selectVar function 

These options can be changed. For example, to compute the expanded set of
performance measures shown above,

```{r}
newRF <- rfFuncs
newRF$summary <- fiveStats
```
To run the RFE procedure for random forests, the syntax is
```{r}
## The control function is similar to trainControl():
ctrl <- rfeControl(method = "repeatedcv",
repeats = 5,
verbose = TRUE,
functions = newRF,
index = index)

set.seed(721)
rfRFE <- rfe(x = training[, predVars],
y = training$Class,
sizes = varSeq,
metric = "ROC",
rfeControl = ctrl,
## now pass options to randomForest()
ntree = 1000)
rfRFE
```

The process for predicting new samples is straightforward:
```{r}
predict(rfRFE, head(testing))
```
The built-in functions predict the classes and probabilities for classification.
There are also built-in functions to do recursive feature selection for models
that require retuning at each iteration. For example, to fit support vector
machines:
```{r}
svmFuncs <- caretFuncs
svmFuncs$summary <- fivestats
ctrl <- rfeControl(method = "repeatedcv",
repeats = 5,
verbose = TRUE,
functions = svmFuncs,
index = index)

set.seed(721)
svmRFE <- rfe(x = training[, predVars],
y = training$Class,
sizes = varSeq,
metric = "ROC",
rfeControl = ctrl,
## Now options to train()
method = "svmRadial",
tuneLength = 12,
preProc = c("center", "scale"),
## Below specifies the inner resampling process
trControl = trainControl(method = "cv",
verboseIter = FALSE,
classProbs = TRUE))
svmRFE
```
Here we can see that the poor performance is related to the class imbalance;
the model is biased towards high specificity since most samples are controls.
The caret web page contains more details and examples related to rfe.

# Filter Methods
caret has a function called sbf (for Selection By Filter) that can be used to
screen predictors for models and to estimate performance using resampling.
Any function can be written to screen the predictors.
For example, to compute a p-value for each predictor, depending on the
data type, the following approach could be used:
```{r}
pScore <- function(x, y)
  {
  numX <- length(unique(x))
  if(numX > 2)
    {
    ## With many values in x, compute a t-test
    out <- t.test(x ~ y)$p.value
    } else {
      ## For binary predictors, test the odds ratio == 1 via
      ## Fisher's Exact Test
      out <- fisher.test(factor(x), y)$p.value
      }
  out
  }

## Apply the scores to each of the predictor columns
scores <- apply(X = training[, predVars],MARGIN = 2,FUN = pScore, y = training$Class)
tail(scores)

```
A function can also be designed to apply a p-value correction, such as the
Bonferroni procedure:

```{r}
pCorrection <- function (score, x, y)
  {
  ## The options x and y are required by the caret package
  ## but are not used here
  score <- p.adjust(score, "bonferroni")
  ## Return a logical vector to decide which predictors
  ## to retain after the filter
  keepers <- (score <= 0.05)
  keepers
  }
tail(pCorrection(scores))

```

As before, caret contains a number of built-in functions for filter methods:
linear regression, random forests, bagged trees, linear discriminant analysis,
and na¨ıve Bayes (see ?rfSBF for more details). For example, ldaSBF has the
following functions:

```{r}
str(ldaSBF)

```
These functions are similar to those shown for rfe. The score function computes some quantitative measure of importance (e.g., the p-values produced by the previous pScore function). The function filter takes these values (and the raw training set data) and determines which predictors pass the filter.
For the biomarker data, the filtered LDA model was fit using

```{r}
ldaWithPvalues <- ldaSBF
ldaWithPvalues$score <- pScore
ldaWithPvalues$summary <- fiveStats
ldaWithPvalues$filter <- pCorrection
sbfCtrl <- sbfControl(method = "repeatedcv",
repeats = 5,
verbose = TRUE,
functions = ldaWithPvalues,
index = index)

ldaFilter <- sbf(training[, predVars],
training$Class,
tol = 1.0e-12,
sbfControl = sbfCtrl)
ldaFilter

```
Again, the caret package web site has additional detail regarding the rfe
and sbf functions, including features not shown here.
