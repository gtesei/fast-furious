Subset Selection Methods
========================================================

Best Subset Selection
-------------------------

```{r}
library(ISLR)
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
library(leaps)
regfit.full = regsubsets(Salary ~ . , Hitters)
summary(regfit.full)

regfit.full=regsubsets(Salary ~ . ,data=Hitters ,nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ",ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type="l")
which.min(reg.summary$cp )
points(10,reg.summary$cp [10],col="red",cex=2,pch=20)
which.min(reg.summary$bic )
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points(6,reg.summary$bic [6],col="red",cex=2,pch=20)
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full ,6)

```


Forward and Backward Stepwise Selection
-------------------------

```{r}
regfit.fwd=regsubsets (Salary~.,data=Hitters ,nvmax=19, method ="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets (Salary~.,data=Hitters ,nvmax=19,
method ="backward")
summary(regfit.bwd)
coef(regfit.full ,7)
coef(regfit.fwd ,7)
coef(regfit.bwd ,7)
```


Choosing Among Models Using the Validation Set Approach and Cross-Validation
-------------------------

```{r}
set.seed (1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test =(!train)
regfit.best=regsubsets(Salary~.,data=Hitters[train,], nvmax =19)
summary(regfit.best)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
val.errors=rep(NA,19)

for(i in 1:19){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}

val.errors
min(val.errors)
which.min(val.errors)
coef(regfit.best ,10)

predict.regsubsets =function (object ,newdata ,id ,...){
  form=as.formula(object$call [[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi 
}

regfit.best=regsubsets(Salary~.,data=Hitters ,nvmax=19)
coef(regfit.best ,10)

##### k-fold
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

for(j in 1:k) {
  best.fit = regsubsets(Salary ~ . , data = Hitters[folds!=j,], nvmax = 19)
  for (i in 1:19) {
    pred = predict(best.fit , Hitters[folds==j,] , id = i)
    cv.errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)
  } 
}

mean.cv.errors=apply(cv.errors ,2,mean)
mean.cv.errors
min(mean.cv.errors)
m = which.min(mean.cv.errors)
as.numeric(m)
par(mfrow=c(1,1))
plot(mean.cv.errors ,type="b")
reg.best=regsubsets (Salary~.,data=Hitters , nvmax=19)
coef(reg.best ,11)
```

Ridge Regression 
-------------------------

```{r}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda [50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
ridge.mod$lambda [60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,]
```

### xval 
We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso.

```{r}
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

### k-fold
In general, instead of arbitrarily choosing λ = 4, it would be better to use cross-validation to choose the tuning parameter λ. We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument folds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random. 

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
```

This represents a further improvement over the test MSE that we got using λ = 4. Finally, we refit our ridge regression model on the full data set, using the value of λ chosen by cross-validation, and examine the coefficient estimates.

```{r}
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
```

Lasso Regression
-------------------------
In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument **alpha=1**. Other than that change, we proceed just as we did in fitting a ridge model.

```{r}
lasso.mod=glmnet(x[train ,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
```

### xval 
We now perform cross-validation and compute the associated test error.

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
```

So the lasso model with λ chosen by cross-validation contains only seven variables.

```{r}
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
```

### k-fold?
```{r}
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
```

Non-linear Modeling
-------------------------

```{r}
library (ISLR)
attach (Wage)
fit=lm(wage~poly(age ,4) ,data=Wage)
coef(summary (fit))
fit2=lm(wage~poly(age ,4, raw =T),data=Wage)
coef(summary (fit2))
fit2a=lm(wage ~ age+I(age ^2)+I(age ^3)+I(age ^4) ,data=Wage)
coef(fit2a)
fit2b=lm(wage~cbind(age ,age ^2, age ^3, age ^4) ,data=Wage)



```

