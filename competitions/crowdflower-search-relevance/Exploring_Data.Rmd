# Exploring the Crowdflower Data

We'll start by using the readr library to read in the train and test sets
```{r}
getBasePath = function (type = "data") {
  ret = ""
  base.path1 = ""
  base.path2 = ""
  
  if(type == "data") {
    base.path1 = "C:/docs/ff/gitHub/fast-furious/dataset/crowdflower-search-relevance"
    base.path2 = "/Users/gino/kaggle/fast-furious/gitHub/fast-furious/dataset/crowdflower-search-relevance/"
  } else if (type == "code") {
    base.path1 = "C:/docs/ff/gitHub/fast-furious/competitions/crowdflower-search-relevance"
    base.path2 = "/Users/gino/kaggle/fast-furious/gitHub/fast-furious/competitions/ocrowdflower-search-relevance/"
  } else if (type == "process") {
    base.path1 = "C:/docs/ff/gitHub/fast-furious/data_process"
    base.path2 = "/Users/gino/kaggle/fast-furious/gitHub/fast-furious/data_process/"
  } else {
    stop("unrecognized type.")
  }
  
  if (file.exists(base.path1))  {
    ret = paste0(base.path1,"/")
  } else {
    ret = base.path2
  }
  
  ret
}

library(readr)

train <- read_csv(paste(getBasePath("data") , "train.csv" , sep=''))
test  <- read_csv(paste(getBasePath("data") , "test.csv" , sep=''))
```
We can see that train has 6 columns. median_relevance is the one that this competition's trying to predict.
```{r}
names(train)
```
On the other hand, test has 4 columns.
```{r}
names(test)
```
Test doesn't have the two columns that provide direct information on the target variable.
```{r}
setdiff(names(train), names(test))
```
Let's look at the number of rows in the training and test sets.
```{r}
nrow(train)
nrow(test)
```
Here, the training set has about 10k rows, and the test set is about twice as big as the training set.

Let's look at the queries in the train set now.
```{r}
unique(train$query)[1:10]
# The number of unique train queries
length(unique(train$query))
# The number of unique test queries
length(unique(test$query))
# Are any queries different between the sets?
length(setdiff(unique(train$query), unique(test$query)))
```
It looks like all the queries we see in the training set are also in the test set.

Now let's look at the product titles
```{r}
unique(train$product_title)[1:10]
# The number of unique product titles in the training set
length(unique(train$product_title))
# The number of product titles that are only in the train set or only in the test set
length(setdiff(unique(train$product_title), unique(test$product_title)))
# The number of product titles that are in both the train and test sets
length(intersect(unique(train$product_title), unique(test$product_title)))

pt.comm = intersect(unique(train$product_title), unique(test$product_title))
dd = data.frame(product_title = pt.comm[1] , 
                train.query = train[train$product_title %in% pt.comm[1]  , ]$query , 
                train.pd = train[train$product_title %in% pt.comm[1]  , ]$product_description , 
                test.query = test[test$product_title %in% pt.comm[1]  , ]$query, 
                test.pd = test[test$product_title %in% pt.comm[1]  , ]$product_description )

dd

```
This tells us that we only see most of the product titles once, and that the product titles are mostly different between the train and test sets.

Now let's start with some basic text analysis on the queries. First, we'll create a helper function
```{r}
# We'll use the library ggvis for data visualization
library(ggvis)
# And the library tm to help with text processing
library(tm)

# Creating a function plot_word_counts to plot counts of word occurences in different sets
plot_word_counts <- function(documents) {
  # Keep only unique documents and convert them to lowercase
  corpus <- Corpus(VectorSource(tolower(unique(documents))))
  # Remove punctuation from the documents
  corpus <- tm_map(corpus, removePunctuation)
  # Remove english stopwords, such as "the" and "a"
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  doc_terms <- DocumentTermMatrix(corpus)
  doc_terms <- as.data.frame(as.matrix(doc_terms))
  word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
  # Sort from the most frequent words to the least frequent words
  word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
  
  top_words <- word_counts[1:10,]
  top_words$Words <- factor(top_words$Words, levels=top_words$Words)
  
  # Plot the 10 most frequent words with ggvis
  top_words %>%
    ggvis(~Words, ~Counts) %>%
    layer_bars(fill:="#20beff")
}
```
Now, we'll apply that function to look at the most common terms in the query, the product title, and the product description.
```{r}
# The top words in the query
plot_word_counts(c(train$query, test$query))
# The top words in the product title (from a random sample for computational reasons)
set.seed(0)
plot_word_counts(sample(c(train$product_title, test$product_title), 1000))
# The top words in the product description (from a random sample for computational reasons)
plot_word_counts(sample(c(train$product_description, test$product_description), 1000))
```