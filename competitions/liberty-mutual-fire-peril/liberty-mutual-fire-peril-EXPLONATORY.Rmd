---
title: "liberty-mutual-fire-peril- Smanettamenti generali"
output: html_document
---

Functions 
```{r}

getPvalueTypeIError = function(x,y) {
  test = NA
  pvalue = NA
  
  ## type casting and understanding stat test 
  if (class(x) == "integer") x = as.numeric(x)
  if (class(y) == "integer") y = as.numeric(y)
  
  if ( class(x) == "factor" & class(y) == "numeric" ) {
    test = "ANOVA"
  } else if (class(x) == "factor" & class(y) == "factor" ) {
    test = "CHI-SQUARE"
  } else if (class(x) == "numeric" & class(y) == "numeric" ) {
    test = "PEARSON"
  }  else {
    #stop ("class x and class y not supported.")
    test = "ANOVA"
    tmp = x 
    x = y 
    y = tmp 
  }
  
  ## performing stat test and computing p-value
  if (test == "ANOVA") {                
    test.anova = aov(y~x)
    pvalue = summary(test.anova)[[1]][["Pr(>F)"]][1]
  } else if (test == "CHI-SQUARE") {    
    test.chisq = chisq.test(x = x , y = y)
    pvalue = test.chisq$p.value
  } else {                             
    ###  PEARSON
    test.corr = cor.test(x =  x , y =  y)
    pvalue = test.corr$p.value
  }
    
  pvalue
}

getPvalueFeatures = function(response,features) {
  
  pValue <- rep(NA, dim(features)[2])
  is.na <- rep(NA, dim(features)[2])
  
  for (i in 1:(dim(features)[2])) {
    #print(i)
    pValue[i] <- getPvalueTypeIError(x = features[,i], y = response)
    is.na[i] = sum(is.na(features[,i])) / length(features[,i]) 
  }
    
  is.significant = ifelse(pValue < 0.05,T,F)
  data.frame(name = names(features), pValue , is.significant , is.na)
}

kfolds = function(k,data.length) {
  k = min(k,data.length)
  folds = rep(NA,data.length)
  labels = 1:data.length
  st = floor(data.length/k)
  al_labels = NULL
  for (s in 1:k) {
    x = NULL
    if (is.null(al_labels))
      x = sample(labels,st)
    else
      x = sample(labels[-al_labels],st)
    
    folds[x] = s
    if (is.null(al_labels))
      al_labels = x
    else
      al_labels = c(al_labels,x)
    }
  ss = 1
  for (s in 1:length(folds)){
    if (is.na(folds[s])) {
      folds[s] = ss
      ss = ss + 1
      } 
    }
  folds
}

```

Loading data sets (train, test, sample) ... 

```{r , warning=FALSE}
#base.path = "C:/docs/ff/gitHub/fast-furious/dataset/liberty-mutual-group/"
base.path = "/Users/gino/kaggle/fast-furious/gitHub/fast-furious/dataset/liberty-mutual-fire-peril/"

train.fn = "train.csv"
test.fn = "test.csv"
sampleSub.fn = "sampleSubmission.csv"

library(data.table)
train = fread(paste(base.path,train.fn,sep="") , header = TRUE , sep=","  )

dim(train)  ## 452061    302
#dim(train_no_NA)  # 452061    302
sum(train$target == 0) / dim(train)[1] ## 0.997372
sum(train$target == 0) ##450873 
sum(train$target != 0) ##1188

y = train$target
plot(density(y))

density(y)

density(y[y != 0])

min(y) #0
max(y) #25.92014 
mean(y) # 0.00723407
sd(y) # 0.2196884 

min(y[y != 0]) #0.005770778
plot(density(y[y != 0]))
mean(y[y != 0]) #2.752728
sd(y[y != 0]) # 3.288865
```

Performing data trasformation 

```{r}
train = as.data.frame.matrix(train) 
## set NAs
train$var1 = ifelse(train$var1 == "Z" , NA , train$var1)
train$var2 = ifelse(train$var2 == "Z" , NA , train$var2)
train$var3 = ifelse(train$var3 == "Z" , NA , train$var3)
train$var4 = ifelse(train$var4 == "Z" , NA , train$var4)
train$var5 = ifelse(train$var5 == "Z" , NA , train$var5)
train$var6 = ifelse(train$var6 == "Z" , NA , train$var6)
train$var7 = ifelse(train$var7 == "Z" , NA , train$var7)
train$var8 = ifelse(train$var8 == "Z" , NA , train$var8)
train$var9 = ifelse(train$var9 == "Z" , NA , train$var9)

## set correct classes for regression 
train$var1 = as.numeric(train$var1)
train$var2 = as.factor(train$var2)
train$var3 = as.factor(train$var3)
train$var4 = as.factor(train$var4) ## TODO BETTER 
train$var5 = as.factor(train$var5)
train$var6 = as.factor(train$var6)
train$var7 = as.numeric(train$var7)
train$var8 = as.numeric(train$var8)
train$var9 = as.factor(train$var9)
train$dummy = as.factor(train$dummy)

train$target_0 = as.factor(ifelse(train$target == 0,0,1))

## exploratory
pvalues = getPvalueFeatures( features = train[ , - 2] , response = train$target_0 )
pvalues[order(pvalues$pValue) , ]
```

Trying linear regression (k-fold) on target (quantitative response variable) with different set of predicors ... 

```{r}
## linear model with correlated variables (pvalue < 0.05)
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
    lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227 + weatherVar235 + geodemVar37 + weatherVar47 + geodemVar24 + geodemVar20 + geodemVar13 + geodemVar17 + geodemVar8 + geodemVar26 + var11 + geodemVar11  , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01389263 

## all zeros model 
mean(abs(train$target)) ## 0.00723407

## linear model with correlated variables (pvalue < 0.05) & some other ... 
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227 + weatherVar235 + geodemVar37 + weatherVar47 + geodemVar24 + geodemVar20 + geodemVar13 + geodemVar17 + geodemVar8 + geodemVar26 + var11 + geodemVar11 + weatherVar61 + weatherVar104 + weatherVar143 + weatherVar54 + weatherVar174 + weatherVar73 + weatherVar177 + weatherVar165  , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01388663

## linear model with some correlated variables (pvalue < 0.05)  
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01379796

## linear model with top 5 correlated variables (pvalue < 0.05)  
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy  , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01379796

## regression trees with top 5 correlated variables (pvalue < 0.05)  
library(rpart)
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = rpart(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01443015 

library(rpart.plot)
rpart.plot(lm.full,digits=3)

## regression trees with top 5 correlated variables (pvalue < 0.05)  
library(RWeka)
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = M5P(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  pred.xval = predict(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ### 0.01459

## rneural networks  with top 5 correlated variables (pvalue < 0.05)  
library(neuralnet)
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  lm.full = neuralnet(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  pred.xval = compute(lm.full , xvaldata)
  pred.xval = ifelse(is.na(pred.xval) , 0 , pred.xval)
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ## Error in neurons[[i]] %*% weights[[i]] : requires numeric/complex matrix/vector arguments 
                    ## neuralnet only deals with quantitative variables

## classifier (target == 0) +  regressor   with top 5 correlated variables (pvalue < 0.05)  
k = 5
folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  
  lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  
  target0.class = glm(target_0 ~ var11 + var4 + var13+var10 + dummy   , data = traindata , family = "binomial")
  
  pred.class = predict(target0.class , xvaldata , type = "response")
  pred.class = ifelse(is.na(pred.class) , 0 , pred.class)
  pred.class = ifelse( pred.class < 0.5 , 0 , 1)
  
  pred.reg.xval = predict(lm.full , xvaldata)
  pred.reg.xval = ifelse(is.na(pred.reg.xval) , 0 , pred.reg.xval)
  
  pred.xval = ifelse(pred.class == 0 , 0 , pred.reg.xval)
  
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ## 0.007234069512

## classifier (target == 0) +  regressor   with top 5 correlated variables (pvalue < 0.05)  
library(class)
k = 5
trainknn = data.frame(train$var13,train$var10,train$var4,train$var8,train$dummy, train$weatherVar118,train$weatherVar102,train$weatherVar103,train$weatherVar227)

label = train$target_0

folds = kfolds(k,dim(train)[1])
cv.mae=matrix(NA,k,1, dimnames=list(NULL, paste(1:1)))
for(j in 1:k) {  
  traindata = train[folds != j,]
  xvaldata = train[folds == j,]
  
  traindata_knn = trainknn[folds != j,]
  label_knn = trainknn[folds != j,]
  xvaldata_knn = trainknn[folds == j,]
  
  lm.full = lm(target ~ var13 + var10 + var4 + var8 + dummy + weatherVar118 + weatherVar102 + weatherVar103 + weatherVar227   , data = traindata)
  
  pred.class = knn(traindata_knn,xvaldata_knn,label_knn,5)
  pred.class = ifelse(is.na(pred.class) , 0 , pred.class)
  
  pred.reg.xval = predict(lm.full , xvaldata)
  pred.reg.xval = ifelse(is.na(pred.reg.xval) , 0 , pred.reg.xval)
  
  pred.xval = ifelse(pred.class == 0 , 0 , pred.reg.xval)
  
  cv.mae[j,1] = mean(abs((xvaldata$target-pred.xval)))
}
mean.cv.mae=apply(cv.mae ,2,mean)
print(mean.cv.mae)  ## Error in knn(traindata_knn, xvaldata_knn, label_knn, 5) :  no missing values are allowed

```

Some observations 

* all zeros model performs better (mae = 0.00723407) than any other linear regression model 
* model with top 9 predictors performs better than model with 28 predictors (mae = 0.01388663), that on turn performs better than model with top 21 predictors (mae = 0.01389263)
* above behaviour suggest that it's probably better before classify 0s target vs non-0s target and then predict target for non-0s target 
* there's a best minimal set of predictors with best mae to discover with cross validation  
* predictors with is.na > 0.01 have been discarded 

