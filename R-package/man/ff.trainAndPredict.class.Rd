% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/fastClassification.R
\name{ff.trainAndPredict.class}
\alias{ff.trainAndPredict.class}
\title{Trains a specified classification model on the given train set and predicts on the given test set.}
\usage{
ff.trainAndPredict.class(Ytrain, Xtrain, Xtest, model.label, controlObject,
  best.tuning = FALSE, verbose = FALSE,
  removePredictorsMakingIllConditionedSquareMatrix_forLinearModels = TRUE,
  metric.label = "auc", xgb.metric.fun = NULL, xgb.maximize = FALSE,
  xgb.foldList = NULL, xgb.eta = NULL, xgb.max_depth = NULL,
  xgb.cv.default = TRUE, xgb.param = NULL, ...)
}
\arguments{
\item{Ytrain}{the output variable as numeric vector}

\item{Xtrain}{the encoded \code{data.frame} of train data. Must be a \code{data.frame} of \code{numeric}}

\item{Xtest}{the encoded \code{data.frame} of test data. Must be a \code{data.frame} of \code{numeric}}

\item{model.label}{a string specifying which model to use.}

\item{controlObject}{a list of values that define how this function acts. Must be a caret \code{trainControl} object
for all models except that for \code{'xgbTreeGTJ'}.}

\item{best.tuning}{\code{TRUE} to use more dense tuning grid or custom routine/tuning grid if available}

\item{verbose}{\code{TRUE} to enable verbose mode.}

\item{removePredictorsMakingIllConditionedSquareMatrix_forLinearModels}{\code{TRUE} for removing predictors making
ill-conditioned square matrices in case of fragile linear models.}

\item{metric.label}{the label of function to optmize/minimize.}

\item{xgb.metric.fun}{custom function to optmize/minimize for \code{'xgbTreeGTJ'}.}

\item{xgb.maximize}{\code{TRUE} to maximize the specified \code{xgb.metric.fun}.}

\item{xgb.foldList}{custom resampling folds list for \code{'xgbTreeGTJ'}.}

\item{xgb.eta}{custom \code{eta} parameter for \code{'xgbTreeGTJ'}.}

\item{xgb.max_depth}{custom \code{max_depth} parameter for \code{'xgbTreeGTJ'}.}

\item{xgb.cv.default}{\code{TRUE} for using \code{xgboost::xgb.cv} function (mandatory in case of fix nrounds), \code{FALSE} for using the internal
\code{ff.xgb.cv} function. The main advantage of the latter is that it doesn't need to restart nrounds in case for the specified nrounds
cross validation error is still decreasing.}

\item{xgb.param}{custom parameters for XGBoost.}

\item{...}{arguments passed to the regression routine.}
}
\value{
a list of test predictions, model and number of excecuting seconds.
}
\description{
Trains a specified classification model on the given train set and predicts on the given test set.
}
\examples{
## suppress warnings raised because of few obs
warn_def = getOption('warn')
options(warn=-1)

## data
Xtrain <- data.frame( a = rep(1:10 , each = 2), b = 20:1,
                      c = rep(as.Date(c("2007-06-22", "2004-02-13")),10) , d = 20:1)
Xtest <- data.frame( a = rep(2:11 , each = 2), b = 1:20,
                     c = rep(as.Date(c("2007-03-01", "2004-05-23")),10) , d = 1:20)
Ytrain = c(rep(1,10),rep(0,10))

## encode datasets
l = ff.makeFeatureSet(Xtrain,Xtest,c("C","N","D","N"))
Xtrain = l$traindata
Xtest = l$testdata

## make a caret control object
controlObject <- trainControl(method = "repeatedcv", repeats = 2, number = 3 ,
                              summaryFunction = twoClassSummary , classProbs = TRUE)
tp = ff.trainAndPredict.class(Ytrain=Ytrain ,
                             Xtrain=Xtrain ,
                             Xtest=Xtest,
                             model.label = "svmRadial" ,
                             controlObject=controlObject,
                             verbose=TRUE ,
                             best.tuning=TRUE)

pred_test = tp$pred
model = tp$model
elapsed.secs = tp$secs

bestTune = l$model$bestTune
best_ROC = max(tp$model$results$ROC)

## restore warnings
options(warn=warn_def)
}

