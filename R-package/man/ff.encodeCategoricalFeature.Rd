% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/featureEncode.R
\name{ff.encodeCategoricalFeature}
\alias{ff.encodeCategoricalFeature}
\title{Encode a generic predictor as a categorical features using both observations of train set and test for levels. 
It's anyway possible to adopt more levels by using the parameter levels. 
Notice that modeling a generic vector, e.g. \code{c(1,2,3,4,5,2,3)} as a categorical predictor xor a numeric predictor is a 
modeling choice (eventually to be assessed by cross-validation).}
\usage{
ff.encodeCategoricalFeature(data.train, data.test, colname.prefix,
  asNumericSequence = F, replaceWhiteSpaceInLevelsWith = NULL,
  levels = NULL, remove1DummyVar = FALSE)
}
\arguments{
\item{data.train}{the observations of the predictor in train set.}

\item{data.test}{the observations of the predictor in test set.}

\item{colname.prefix}{the prefix of output data frame.}

\item{asNumericSequence}{set \code{T} if the predictor is a numeric sequence filling any possible hole between min and max in observations that could occour both in train set and test set.}

\item{replaceWhiteSpaceInLevelsWith}{replace possible spaces in the train/test name of feature.}

\item{levels}{the levels of the categorical feature. Must be \code{NULL} if asNumericSequence is \code{T}.}

\item{remove1DummyVar}{\code{T} to remove one dummy variable. Why? 
First, if you know the values of the first C - 1 dummy variables, you know the last one too and it is more economical to use C - 1. 
Secondly, if the model has slopes and intercepts (e.g. linear regression), the sum of all of the dummy variables wil add up to the 
intercept (usually encoded as a "1") and that is bad for the math involved. On the other hand, there are models like penalized methods (such as ridge regression) 
that seldom penalize the intercept, so a C-1 encoded variable could cause the other category effects to be penalized towards the reference category effect.}
}
\value{
the list of trainset and testset after applying the specified filters
}
\description{
Encode a generic predictor as a categorical features using both observations of train set and test for levels. 
It's anyway possible to adopt more levels by using the parameter levels. 
Notice that modeling a generic vector, e.g. \code{c(1,2,3,4,5,2,3)} as a categorical predictor xor a numeric predictor is a 
modeling choice (eventually to be assessed by cross-validation).
}
\examples{
Xtrain <- data.frame( a = rep(1:3 , each = 2), b = 6:1, c = letters[1:6])
Xtest <- data.frame( a = rep(2:4 , each = 2), b = 1:6, c = letters[6:1])
print(Xtrain)
#   a b c
# 1 1 6 a
# 2 1 5 b
# 3 2 4 c
# 4 2 3 d
# 5 3 2 e
# 6 3 1 f

l = ff.encodeCategoricalFeature (Xtrain$c , Xtest$c , "c")
l$traindata
#     c_1 c_2 c_3 c_4 c_5 c_6
# 7    1   0   0   0   0   0
# 8    0   1   0   0   0   0
# 9    0   0   1   0   0   0
# 10   0   0   0   1   0   0
# 11   0   0   0   0   1   0
# 12   0   0   0   0   0   1

Xtrain[,'c'] = NULL
Xtest[,'c'] = NULL
Xtrain = cbind(Xtrain,l$traindata)
Xtest = cbind(Xtest,l$testdata)
}
\references{
\url{http://appliedpredictivemodeling.com/blog/2013/10/23/the-basics-of-encoding-categorical-data-for-predictive-models}
}

